{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.JPIJNSWNNAN3CE6LLI5FWSPHUT2VXMTH.gfortran-win_amd64.dll\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn import metrics, svm, preprocessing\n",
    "from pandas import DataFrame\n",
    "from keras import backend as K\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from IPython.display import SVG\n",
    "from keras.utils import plot_model\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root = '', MetaFile = 'ADoub_meta.csv', ADoubFile = 'ADoub.npy'):\n",
    "\n",
    "    ADoub_meta = pd.read_csv(root + MetaFile)\n",
    "    X = np.load(root + ADoubFile)\n",
    "    \n",
    "    return X, ADoub_meta\n",
    "\n",
    "def gen_label(Y_raw):\n",
    "    \n",
    "    Y_raw = Y_raw.astype(int)\n",
    "    Y = np.zeros((Y_raw.shape[0], Y_raw.max() + 1))\n",
    "    for i, idx in enumerate(Y_raw):\n",
    "        Y[i, idx] = 1\n",
    "        \n",
    "    return Y\n",
    "\n",
    "def FCNN(input_shape, out_shape, nodes = [2048, 1024, 512]):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = X_input\n",
    "    \n",
    "    for node in nodes:\n",
    "        X = Dense(node, init = 'glorot_normal', activation = 'relu')(X)\n",
    "        X = BatchNormalization(axis = 1)(X)\n",
    "    \n",
    "    X = Dense(out_shape, init = 'glorot_normal', activation = 'softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name = 'FCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def batch_generator(X, y, sample_index, batch_size, shuffle):\n",
    "    number_of_batches = int(np.ceil(len(sample_index)/batch_size))\n",
    "    counter = 0\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index, :]\n",
    "        y_batch = y[batch_index, :]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "            \n",
    "def CrossValidation(X, Y, fold = 5, CV = True, epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512]):\n",
    "\n",
    "    kf = KFold(n_splits=fold, random_state = 0, shuffle = True)\n",
    "    cv_index = kf.split(X)\n",
    "\n",
    "    length = Y.shape[1]\n",
    "\n",
    "    earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "    # Set a learning rate annealer\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=3, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "    \n",
    "    histories = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for cv_train, cv_test in cv_index:\n",
    "\n",
    "        if((not CV) & (i!= 0)):\n",
    "            break\n",
    "        i += 1\n",
    "        print(i)\n",
    "\n",
    "        X_valid = X[cv_test, :].copy()\n",
    "\n",
    "        model = FCNN((X.shape[1],), length, nodes = nodes)\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        history = model.fit_generator(generator = batch_generator(X,\\\n",
    "                                                        Y,\\\n",
    "                                                        cv_train,\\\n",
    "                                                        batch_size, True),\\\n",
    "                            epochs = epoch,\\\n",
    "                            steps_per_epoch = len(cv_train) / batch_size,\\\n",
    "                            validation_data = (X_valid, Y[cv_test, :]),\\\n",
    "                            callbacks = [learning_rate_reduction, earlystopper])\n",
    "        \n",
    "        histories.append(history)\n",
    "\n",
    "        gc.collect()\n",
    "    if CV:\n",
    "        return histories\n",
    "    else:\n",
    "        return history\n",
    "\n",
    "def Train(X, Y, root, name = '1_9_1', epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512]):\n",
    "    \n",
    "    length = Y.shape[1]\n",
    "\n",
    "    earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "    # Set a learning rate annealer\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=3, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "\n",
    "    model = FCNN((X.shape[1],), length, nodes = nodes)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit_generator(generator = batch_generator(X,\\\n",
    "                                                    Y,\\\n",
    "                                                    np.arange(X.shape[0]),\\\n",
    "                                                    batch_size, True),\\\n",
    "                        epochs = epoch,\\\n",
    "                        steps_per_epoch = X.shape[0] / batch_size,\\\n",
    "                        callbacks = [learning_rate_reduction, earlystopper])\n",
    "\n",
    "    model.save(root + 'Decomp_' + name + '.h5')\n",
    "    \n",
    "def SaveValidation(history, CV, name):\n",
    "    \n",
    "    if(CV):\n",
    "        histories = history\n",
    "        \n",
    "        with open(save_root + 'training_log_' + name + '.pickle', 'wb') as handle:\n",
    "            pickle.dump([history.history for history in histories], handle)\n",
    "\n",
    "        with open(save_root + 'training_log_' + name + '.pickle', 'rb') as handle:\n",
    "            histories = pickle.load(handle)\n",
    "\n",
    "        accuracy = []\n",
    "\n",
    "        for history in histories:\n",
    "            accuracy.append(history['val_accuracy'][-1])\n",
    "\n",
    "        print('Validation Accuracy')\n",
    "        print(accuracy)\n",
    "        print(np.mean(accuracy))\n",
    "        \n",
    "    else:\n",
    "        with open(save_root + 'training_log_' + name + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(history, handle)\n",
    "\n",
    "        with open(save_root + 'training_log_' + name + '.pickle', 'rb') as handle:\n",
    "            history = pickle.load(handle)\n",
    "            \n",
    "        print(history.history['val_accuracy'][-1])\n",
    "        \n",
    "def CheckAccuracy(name):\n",
    "    with open(save_root + 'training_log_' + name + '.pickle', 'rb') as handle:\n",
    "        histories = pickle.load(handle)\n",
    "\n",
    "        accuracy = []\n",
    "\n",
    "        for history in histories:\n",
    "            accuracy.append(history['val_accuracy'][-1])\n",
    "\n",
    "        print(name)\n",
    "        print('Validation Accuracy')\n",
    "        print(accuracy)\n",
    "        print(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 1415)\n",
      "(400000, 1203)\n",
      "(400000, 8676)\n",
      "[1000000. 1000000. 1000000. ... 1000000. 1000000. 1000000.]\n"
     ]
    }
   ],
   "source": [
    "file_root = 'ADoub_new/'\n",
    "save_root = 'DecompModel/'\n",
    "\n",
    "if not os.path.exists(save_root):\n",
    "    os.makedirs(save_root)\n",
    "    \n",
    "X, ADoub_meta = load_data(file_root, 'ADoub_meta.csv', 'ADoub.npy')\n",
    "\n",
    "Y1 = gen_label(ADoub_meta.iloc[:, 0])\n",
    "Y2 = gen_label(ADoub_meta.iloc[:, 2])\n",
    "\n",
    "print(Y1.shape)\n",
    "print(Y2.shape)\n",
    "print(X.shape)\n",
    "print(X.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "1\n",
      "**********\n",
      "1\n",
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "79/78 [==============================] - 47s 594ms/step - loss: 0.6698 - accuracy: 0.9398 - val_loss: 4.8046 - val_accuracy: 0.6193\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 564ms/step - loss: 0.0344 - accuracy: 0.9942 - val_loss: 1.8805 - val_accuracy: 0.9874\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 563ms/step - loss: 0.0250 - accuracy: 0.9955 - val_loss: 0.2278 - val_accuracy: 0.9924\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0217 - accuracy: 0.9960 - val_loss: 0.0706 - val_accuracy: 0.9933\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0175 - accuracy: 0.9966 - val_loss: 0.1034 - val_accuracy: 0.9937\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0145 - accuracy: 0.9971 - val_loss: 0.0634 - val_accuracy: 0.9943\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 45s 567ms/step - loss: 0.0126 - accuracy: 0.9976 - val_loss: 0.0448 - val_accuracy: 0.9948\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 46s 577ms/step - loss: 0.0102 - accuracy: 0.9979 - val_loss: 0.0413 - val_accuracy: 0.9952\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 45s 573ms/step - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.0347 - val_accuracy: 0.9952\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.0309 - val_accuracy: 0.9951\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.0275 - val_accuracy: 0.9952\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 46s 587ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.0148 - val_accuracy: 0.9970\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0132 - val_accuracy: 0.9973\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 46s 588ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0123 - val_accuracy: 0.9974\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 47s 589ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0123 - val_accuracy: 0.9975\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 47s 589ms/step - loss: 8.3741e-04 - accuracy: 0.9998 - val_loss: 0.0119 - val_accuracy: 0.9976\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 46s 586ms/step - loss: 9.7070e-04 - accuracy: 0.9998 - val_loss: 0.0116 - val_accuracy: 0.9977\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 45s 576ms/step - loss: 8.1595e-04 - accuracy: 0.9998 - val_loss: 0.0117 - val_accuracy: 0.9977\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 8.2778e-04 - accuracy: 0.9998 - val_loss: 0.0120 - val_accuracy: 0.9977\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 45s 576ms/step - loss: 6.4013e-04 - accuracy: 0.9999 - val_loss: 0.0110 - val_accuracy: 0.9978\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 47s 594ms/step - loss: 0.6651 - accuracy: 0.9398 - val_loss: 5.0256 - val_accuracy: 0.4978\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.0363 - accuracy: 0.9940 - val_loss: 1.4088 - val_accuracy: 0.9880\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.1581 - val_accuracy: 0.9919\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0210 - accuracy: 0.9961 - val_loss: 0.0547 - val_accuracy: 0.9932\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 570ms/step - loss: 0.0184 - accuracy: 0.9966 - val_loss: 0.0479 - val_accuracy: 0.9936\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0150 - accuracy: 0.9972 - val_loss: 0.0509 - val_accuracy: 0.9946\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 47s 592ms/step - loss: 0.0131 - accuracy: 0.9976 - val_loss: 0.0434 - val_accuracy: 0.9945\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 46s 576ms/step - loss: 0.0125 - accuracy: 0.9976 - val_loss: 0.0322 - val_accuracy: 0.9948\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 45s 573ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.0309 - val_accuracy: 0.9950\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 46s 580ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.0301 - val_accuracy: 0.9954\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0272 - val_accuracy: 0.9956\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 45s 576ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0241 - val_accuracy: 0.9959\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 45s 575ms/step - loss: 0.0081 - accuracy: 0.9984 - val_loss: 0.0241 - val_accuracy: 0.9959\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 46s 581ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0218 - val_accuracy: 0.9961\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 46s 584ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0213 - val_accuracy: 0.9966\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 45s 573ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0199 - val_accuracy: 0.9966\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 46s 578ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.0212 - val_accuracy: 0.9962\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 46s 577ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.0188 - val_accuracy: 0.9967\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0122 - val_accuracy: 0.9976\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 46s 579ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0118 - val_accuracy: 0.9977\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 47s 596ms/step - loss: 0.6627 - accuracy: 0.9405 - val_loss: 4.9751 - val_accuracy: 0.5627\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0351 - accuracy: 0.9941 - val_loss: 1.1062 - val_accuracy: 0.9892\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 570ms/step - loss: 0.0255 - accuracy: 0.9955 - val_loss: 0.1732 - val_accuracy: 0.9938\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0203 - accuracy: 0.9962 - val_loss: 0.0622 - val_accuracy: 0.9943\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 576ms/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.0443 - val_accuracy: 0.9952\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0156 - accuracy: 0.9970 - val_loss: 0.0400 - val_accuracy: 0.9952\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 46s 581ms/step - loss: 0.0133 - accuracy: 0.9974 - val_loss: 0.0312 - val_accuracy: 0.9953\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 46s 586ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.0267 - val_accuracy: 0.9959\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.0255 - val_accuracy: 0.9958\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 46s 578ms/step - loss: 0.0085 - accuracy: 0.9983 - val_loss: 0.0212 - val_accuracy: 0.9965\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 46s 578ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.0269 - val_accuracy: 0.9966\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 46s 581ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0229 - val_accuracy: 0.9966\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 46s 577ms/step - loss: 0.0075 - accuracy: 0.9984 - val_loss: 0.0239 - val_accuracy: 0.9968\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 46s 580ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0254 - val_accuracy: 0.9965\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 47s 590ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0204 - val_accuracy: 0.9968\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 46s 585ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.0233 - val_accuracy: 0.9965\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0134 - val_accuracy: 0.9977\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 46s 578ms/step - loss: 8.6957e-04 - accuracy: 0.9998 - val_loss: 0.0119 - val_accuracy: 0.9981\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 46s 580ms/step - loss: 8.1618e-04 - accuracy: 0.9998 - val_loss: 0.0114 - val_accuracy: 0.9981\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 8.5299e-04 - accuracy: 0.9998 - val_loss: 0.0107 - val_accuracy: 0.9983\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 47s 594ms/step - loss: 0.6649 - accuracy: 0.9401 - val_loss: 5.0228 - val_accuracy: 0.4560\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 564ms/step - loss: 0.0368 - accuracy: 0.9940 - val_loss: 1.5199 - val_accuracy: 0.9863\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0259 - accuracy: 0.9957 - val_loss: 0.1695 - val_accuracy: 0.9932\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.0212 - accuracy: 0.9963 - val_loss: 0.0754 - val_accuracy: 0.9933\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0177 - accuracy: 0.9968 - val_loss: 0.0892 - val_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 566ms/step - loss: 0.0159 - accuracy: 0.9971 - val_loss: 0.0804 - val_accuracy: 0.9945\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0137 - accuracy: 0.9972 - val_loss: 0.0368 - val_accuracy: 0.9951\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.0104 - accuracy: 0.9978 - val_loss: 0.0466 - val_accuracy: 0.9953\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 46s 581ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.0391 - val_accuracy: 0.9958\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.0287 - val_accuracy: 0.9955\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.0489 - val_accuracy: 0.9960\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 45s 567ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9963\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.0075 - accuracy: 0.9986 - val_loss: 0.0430 - val_accuracy: 0.9962\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 46s 583ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0274 - val_accuracy: 0.9962\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0518 - val_accuracy: 0.9962\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 46s 584ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0165 - val_accuracy: 0.9973\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0150 - val_accuracy: 0.9978\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0144 - val_accuracy: 0.9978\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 9.8106e-04 - accuracy: 0.9998 - val_loss: 0.0151 - val_accuracy: 0.9978\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 8.8732e-04 - accuracy: 0.9998 - val_loss: 0.0143 - val_accuracy: 0.9980\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.6669 - accuracy: 0.9397 - val_loss: 5.0842 - val_accuracy: 0.5830\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 564ms/step - loss: 0.0367 - accuracy: 0.9940 - val_loss: 1.2119 - val_accuracy: 0.9897\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0255 - accuracy: 0.9954 - val_loss: 0.2275 - val_accuracy: 0.9930\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0217 - accuracy: 0.9960 - val_loss: 0.0690 - val_accuracy: 0.9938\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.0162 - accuracy: 0.9969 - val_loss: 0.0446 - val_accuracy: 0.9939\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 566ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 0.0478 - val_accuracy: 0.9944\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 44s 561ms/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 0.0562 - val_accuracy: 0.9948\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 45s 571ms/step - loss: 0.0099 - accuracy: 0.9979 - val_loss: 0.0415 - val_accuracy: 0.9953\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 46s 580ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.0331 - val_accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 46s 578ms/step - loss: 0.0082 - accuracy: 0.9984 - val_loss: 0.0246 - val_accuracy: 0.9953\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 46s 586ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.0285 - val_accuracy: 0.9956\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.0278 - val_accuracy: 0.9959\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 46s 576ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.0219 - val_accuracy: 0.9958\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 46s 580ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.0778 - val_accuracy: 0.9961\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0250 - val_accuracy: 0.9965\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 46s 583ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.0202 - val_accuracy: 0.9964\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0209 - val_accuracy: 0.9967\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 46s 587ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0234 - val_accuracy: 0.9964\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0218 - val_accuracy: 0.9965\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 45s 572ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0217 - val_accuracy: 0.9965\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Validation Accuracy\n",
      "[0.9977874755859375, 0.9976999759674072, 0.998324990272522, 0.99795001745224, 0.9965375065803528]\n",
      "0.9976599931716919\n"
     ]
    }
   ],
   "source": [
    "#CrossValidation\n",
    "main_name = 'hep_LEC'\n",
    "##1\n",
    "print('*' * 10)\n",
    "print(1)\n",
    "print('*' * 10)\n",
    "\n",
    "name = main_name + '_1'\n",
    "\n",
    "CV = True\n",
    "\n",
    "history = CrossValidation(X, Y1, fold = 5, CV=CV, epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512])\n",
    "\n",
    "SaveValidation(history, CV, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hep_LEC_1\n",
      "Validation Accuracy\n",
      "[0.9977874755859375, 0.9976999759674072, 0.998324990272522, 0.99795001745224, 0.9965375065803528]\n",
      "0.9976599931716919\n"
     ]
    }
   ],
   "source": [
    "name = main_name + '_1'\n",
    "CheckAccuracy(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "2\n",
      "**********\n",
      "1\n",
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "79/78 [==============================] - 46s 585ms/step - loss: 2.8733 - accuracy: 0.6393 - val_loss: 6.7572 - val_accuracy: 0.0600\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 44s 561ms/step - loss: 0.8640 - accuracy: 0.8675 - val_loss: 5.3085 - val_accuracy: 0.3116\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 44s 557ms/step - loss: 0.5825 - accuracy: 0.8998 - val_loss: 2.2889 - val_accuracy: 0.7111\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 45s 566ms/step - loss: 0.4471 - accuracy: 0.9177 - val_loss: 1.5732 - val_accuracy: 0.7781\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.3718 - accuracy: 0.9289 - val_loss: 1.5119 - val_accuracy: 0.7913\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 44s 557ms/step - loss: 0.3247 - accuracy: 0.9359 - val_loss: 1.6230 - val_accuracy: 0.7926\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 47s 599ms/step - loss: 0.2753 - accuracy: 0.9443 - val_loss: 1.8217 - val_accuracy: 0.7677\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.2538 - accuracy: 0.9484 - val_loss: 1.9590 - val_accuracy: 0.7791\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 46s 582ms/step - loss: 0.2283 - accuracy: 0.9523 - val_loss: 1.5719 - val_accuracy: 0.7980\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 45s 574ms/step - loss: 0.2019 - accuracy: 0.9566 - val_loss: 1.9769 - val_accuracy: 0.7287\n",
      "Epoch 00010: early stopping\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 47s 589ms/step - loss: 2.8774 - accuracy: 0.6394 - val_loss: 6.7632 - val_accuracy: 0.0600\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.8745 - accuracy: 0.8658 - val_loss: 5.2995 - val_accuracy: 0.3029\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 45s 564ms/step - loss: 0.5926 - accuracy: 0.8978 - val_loss: 2.6478 - val_accuracy: 0.6723\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 46s 577ms/step - loss: 0.4449 - accuracy: 0.9180 - val_loss: 1.8489 - val_accuracy: 0.7505\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.3581 - accuracy: 0.9303 - val_loss: 1.8886 - val_accuracy: 0.7466\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.3136 - accuracy: 0.9384 - val_loss: 1.6017 - val_accuracy: 0.7736\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.2755 - accuracy: 0.9445 - val_loss: 1.5502 - val_accuracy: 0.7931\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.2527 - accuracy: 0.9476 - val_loss: 1.8547 - val_accuracy: 0.7772\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 45s 570ms/step - loss: 0.2253 - accuracy: 0.9526 - val_loss: 1.7149 - val_accuracy: 0.7977\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.2097 - accuracy: 0.9557 - val_loss: 1.4548 - val_accuracy: 0.8046\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 45s 569ms/step - loss: 0.2022 - accuracy: 0.9572 - val_loss: 1.7549 - val_accuracy: 0.8135\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.1863 - accuracy: 0.9600 - val_loss: 1.7967 - val_accuracy: 0.7917\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.1713 - accuracy: 0.9631 - val_loss: 1.8109 - val_accuracy: 0.7215\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 45s 566ms/step - loss: 0.1570 - accuracy: 0.9655 - val_loss: 1.3740 - val_accuracy: 0.8411\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 44s 558ms/step - loss: 0.1541 - accuracy: 0.9663 - val_loss: 1.8249 - val_accuracy: 0.7973\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.1354 - accuracy: 0.9696 - val_loss: 1.9849 - val_accuracy: 0.8143\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 44s 556ms/step - loss: 0.1297 - accuracy: 0.9707 - val_loss: 1.2069 - val_accuracy: 0.8547\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1271 - accuracy: 0.9714 - val_loss: 1.2183 - val_accuracy: 0.8486\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 44s 557ms/step - loss: 0.1188 - accuracy: 0.9732 - val_loss: 1.8403 - val_accuracy: 0.8213\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.1133 - accuracy: 0.9743 - val_loss: 2.7796 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 46s 577ms/step - loss: 2.8858 - accuracy: 0.6388 - val_loss: 6.7587 - val_accuracy: 0.0496\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.8646 - accuracy: 0.8669 - val_loss: 5.3934 - val_accuracy: 0.2897\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 44s 551ms/step - loss: 0.5862 - accuracy: 0.8981 - val_loss: 2.0673 - val_accuracy: 0.7229\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.4478 - accuracy: 0.9175 - val_loss: 1.5893 - val_accuracy: 0.7688\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.3669 - accuracy: 0.9298 - val_loss: 1.6790 - val_accuracy: 0.7754\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.3268 - accuracy: 0.9359 - val_loss: 1.6295 - val_accuracy: 0.7762\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.2848 - accuracy: 0.9433 - val_loss: 1.5120 - val_accuracy: 0.7915\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 44s 556ms/step - loss: 0.2476 - accuracy: 0.9493 - val_loss: 1.4130 - val_accuracy: 0.7945\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 44s 558ms/step - loss: 0.2264 - accuracy: 0.9525 - val_loss: 1.6665 - val_accuracy: 0.7958\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 44s 556ms/step - loss: 0.2056 - accuracy: 0.9561 - val_loss: 1.1891 - val_accuracy: 0.8285\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.2065 - accuracy: 0.9566 - val_loss: 2.2965 - val_accuracy: 0.7535\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1875 - accuracy: 0.9600 - val_loss: 2.4017 - val_accuracy: 0.6705\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 45s 565ms/step - loss: 0.1743 - accuracy: 0.9629 - val_loss: 1.6117 - val_accuracy: 0.7957\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 44s 551ms/step - loss: 0.0819 - accuracy: 0.9826 - val_loss: 0.9557 - val_accuracy: 0.8683\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.0398 - accuracy: 0.9909 - val_loss: 0.6333 - val_accuracy: 0.9010\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 43s 546ms/step - loss: 0.0458 - accuracy: 0.9895 - val_loss: 0.7724 - val_accuracy: 0.8950\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 44s 551ms/step - loss: 0.0450 - accuracy: 0.9896 - val_loss: 0.6581 - val_accuracy: 0.9032\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 43s 550ms/step - loss: 0.0489 - accuracy: 0.9887 - val_loss: 0.7217 - val_accuracy: 0.9024\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 43s 547ms/step - loss: 0.0493 - accuracy: 0.9884 - val_loss: 1.2286 - val_accuracy: 0.8355\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.0486 - accuracy: 0.9884 - val_loss: 0.7784 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 00020: early stopping\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 2.8765 - accuracy: 0.6385 - val_loss: 6.6765 - val_accuracy: 0.0572\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 43s 550ms/step - loss: 0.8685 - accuracy: 0.8661 - val_loss: 4.9582 - val_accuracy: 0.3428\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.5813 - accuracy: 0.8985 - val_loss: 2.2696 - val_accuracy: 0.6941\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 44s 552ms/step - loss: 0.4457 - accuracy: 0.9181 - val_loss: 2.0031 - val_accuracy: 0.7501\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.3717 - accuracy: 0.9290 - val_loss: 1.6435 - val_accuracy: 0.7680\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 45s 568ms/step - loss: 0.3156 - accuracy: 0.9378 - val_loss: 1.8599 - val_accuracy: 0.7753\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 44s 558ms/step - loss: 0.2883 - accuracy: 0.9421 - val_loss: 1.6842 - val_accuracy: 0.7906\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.2568 - accuracy: 0.9478 - val_loss: 1.4872 - val_accuracy: 0.7910\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 43s 551ms/step - loss: 0.2295 - accuracy: 0.9523 - val_loss: 1.7196 - val_accuracy: 0.7453\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 43s 549ms/step - loss: 0.2156 - accuracy: 0.9553 - val_loss: 1.7792 - val_accuracy: 0.7535\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1927 - accuracy: 0.9587 - val_loss: 1.1564 - val_accuracy: 0.8309\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1736 - accuracy: 0.9622 - val_loss: 1.5154 - val_accuracy: 0.8018\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.1626 - accuracy: 0.9643 - val_loss: 1.9468 - val_accuracy: 0.7451\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 44s 551ms/step - loss: 0.1567 - accuracy: 0.9657 - val_loss: 1.0792 - val_accuracy: 0.8468\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.1533 - accuracy: 0.9659 - val_loss: 2.5223 - val_accuracy: 0.7606\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1522 - accuracy: 0.9670 - val_loss: 1.2639 - val_accuracy: 0.8415\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.1352 - accuracy: 0.9700 - val_loss: 1.4895 - val_accuracy: 0.8285\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 43s 544ms/step - loss: 0.0580 - accuracy: 0.9874 - val_loss: 0.9093 - val_accuracy: 0.8653\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.0255 - accuracy: 0.9943 - val_loss: 0.6352 - val_accuracy: 0.9026\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 44s 553ms/step - loss: 0.0240 - accuracy: 0.9944 - val_loss: 0.5179 - val_accuracy: 0.9191\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/78 [==============================] - 46s 579ms/step - loss: 2.8799 - accuracy: 0.6396 - val_loss: 6.6253 - val_accuracy: 0.0933\n",
      "Epoch 2/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.8592 - accuracy: 0.8674 - val_loss: 4.7942 - val_accuracy: 0.4675\n",
      "Epoch 3/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.5835 - accuracy: 0.8991 - val_loss: 2.2858 - val_accuracy: 0.7104\n",
      "Epoch 4/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.4444 - accuracy: 0.9180 - val_loss: 1.5755 - val_accuracy: 0.7828\n",
      "Epoch 5/20\n",
      "79/78 [==============================] - 44s 557ms/step - loss: 0.3671 - accuracy: 0.9298 - val_loss: 1.7920 - val_accuracy: 0.7786\n",
      "Epoch 6/20\n",
      "79/78 [==============================] - 44s 556ms/step - loss: 0.3248 - accuracy: 0.9361 - val_loss: 1.4510 - val_accuracy: 0.8023\n",
      "Epoch 7/20\n",
      "79/78 [==============================] - 44s 559ms/step - loss: 0.2784 - accuracy: 0.9436 - val_loss: 1.6804 - val_accuracy: 0.7888\n",
      "Epoch 8/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.2444 - accuracy: 0.9495 - val_loss: 1.5943 - val_accuracy: 0.8058\n",
      "Epoch 9/20\n",
      "79/78 [==============================] - 44s 558ms/step - loss: 0.2405 - accuracy: 0.9503 - val_loss: 1.4166 - val_accuracy: 0.8101\n",
      "Epoch 10/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.2161 - accuracy: 0.9548 - val_loss: 1.8175 - val_accuracy: 0.7670\n",
      "Epoch 11/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.1978 - accuracy: 0.9578 - val_loss: 1.5324 - val_accuracy: 0.8197\n",
      "Epoch 12/20\n",
      "79/78 [==============================] - 44s 554ms/step - loss: 0.1829 - accuracy: 0.9603 - val_loss: 1.5614 - val_accuracy: 0.8135\n",
      "Epoch 13/20\n",
      "79/78 [==============================] - 44s 562ms/step - loss: 0.1707 - accuracy: 0.9632 - val_loss: 1.0460 - val_accuracy: 0.8531\n",
      "Epoch 14/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.1601 - accuracy: 0.9649 - val_loss: 1.9894 - val_accuracy: 0.7497\n",
      "Epoch 15/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.1514 - accuracy: 0.9668 - val_loss: 1.6959 - val_accuracy: 0.8165\n",
      "Epoch 16/20\n",
      "79/78 [==============================] - 44s 563ms/step - loss: 0.1455 - accuracy: 0.9680 - val_loss: 1.3236 - val_accuracy: 0.8397\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 17/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.0612 - accuracy: 0.9867 - val_loss: 1.0138 - val_accuracy: 0.8630\n",
      "Epoch 18/20\n",
      "79/78 [==============================] - 44s 559ms/step - loss: 0.0266 - accuracy: 0.9942 - val_loss: 0.7831 - val_accuracy: 0.8910\n",
      "Epoch 19/20\n",
      "79/78 [==============================] - 44s 560ms/step - loss: 0.0260 - accuracy: 0.9937 - val_loss: 0.6859 - val_accuracy: 0.9099\n",
      "Epoch 20/20\n",
      "79/78 [==============================] - 44s 555ms/step - loss: 0.0312 - accuracy: 0.9926 - val_loss: 0.7387 - val_accuracy: 0.9050\n",
      "Validation Accuracy\n",
      "[0.7287499904632568, 0.7648375034332275, 0.8947250247001648, 0.9191250205039978, 0.9050124883651733]\n",
      "0.8424900054931641\n"
     ]
    }
   ],
   "source": [
    "##2\n",
    "main_name = 'hep_LEC'\n",
    "print('*' * 10)\n",
    "print(2)\n",
    "print('*' * 10)\n",
    "\n",
    "name = main_name + '_2'\n",
    "\n",
    "CV=True\n",
    "\n",
    "history = CrossValidation(X, Y2, fold = 5, CV=CV, epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512])\n",
    "\n",
    "SaveValidation(history, CV, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hep_LEC_2\n",
      "Validation Accuracy\n",
      "[0.7287499904632568, 0.7648375034332275, 0.8947250247001648, 0.9191250205039978, 0.9050124883651733]\n",
      "0.8424900054931641\n"
     ]
    }
   ],
   "source": [
    "name = main_name + '_2'\n",
    "CheckAccuracy(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1415, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "98/97 [==============================] - 39s 393ms/step - loss: 0.5420 - accuracy: 0.9510\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/97 [==============================] - 38s 386ms/step - loss: 0.0313 - accuracy: 0.9946\n",
      "Epoch 3/20\n",
      "98/97 [==============================] - 38s 388ms/step - loss: 0.0224 - accuracy: 0.9959\n",
      "Epoch 4/20\n",
      "98/97 [==============================] - 39s 395ms/step - loss: 0.0168 - accuracy: 0.9967\n",
      "Epoch 5/20\n",
      "98/97 [==============================] - 38s 391ms/step - loss: 0.0142 - accuracy: 0.9972\n",
      "Epoch 6/20\n",
      "98/97 [==============================] - 39s 397ms/step - loss: 0.0115 - accuracy: 0.9977\n",
      "Epoch 7/20\n",
      "98/97 [==============================] - 39s 394ms/step - loss: 0.0094 - accuracy: 0.9981\n",
      "Epoch 8/20\n",
      "98/97 [==============================] - 39s 393ms/step - loss: 0.0084 - accuracy: 0.9982\n",
      "Epoch 9/20\n",
      "98/97 [==============================] - 39s 399ms/step - loss: 0.0075 - accuracy: 0.9984\n",
      "Epoch 10/20\n",
      "98/97 [==============================] - 40s 413ms/step - loss: 0.0073 - accuracy: 0.9985\n",
      "Epoch 11/20\n",
      "98/97 [==============================] - 40s 408ms/step - loss: 0.0063 - accuracy: 0.9987\n",
      "Epoch 12/20\n",
      "98/97 [==============================] - 41s 416ms/step - loss: 0.0061 - accuracy: 0.9987\n",
      "Epoch 13/20\n",
      "98/97 [==============================] - 40s 410ms/step - loss: 0.0055 - accuracy: 0.9988\n",
      "Epoch 14/20\n",
      "98/97 [==============================] - 40s 409ms/step - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 15/20\n",
      "98/97 [==============================] - 40s 408ms/step - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 16/20\n",
      "98/97 [==============================] - 40s 408ms/step - loss: 0.0051 - accuracy: 0.9989\n",
      "Epoch 17/20\n",
      "98/97 [==============================] - 39s 400ms/step - loss: 0.0047 - accuracy: 0.9990\n",
      "Epoch 18/20\n",
      "98/97 [==============================] - 40s 406ms/step - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 19/20\n",
      "98/97 [==============================] - 39s 402ms/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 20/20\n",
      "98/97 [==============================] - 38s 390ms/step - loss: 0.0044 - accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1203, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/97 [==============================] - 38s 390ms/step - loss: 2.5599 - accuracy: 0.6771\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "C:\\Users\\nrx398\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/97 [==============================] - 37s 375ms/step - loss: 0.7473 - accuracy: 0.8795\n",
      "Epoch 3/20\n",
      "98/97 [==============================] - 37s 376ms/step - loss: 0.5026 - accuracy: 0.9097\n",
      "Epoch 4/20\n",
      "98/97 [==============================] - 37s 381ms/step - loss: 0.3850 - accuracy: 0.9269\n",
      "Epoch 5/20\n",
      "98/97 [==============================] - 38s 385ms/step - loss: 0.3176 - accuracy: 0.9371\n",
      "Epoch 6/20\n",
      "98/97 [==============================] - 38s 388ms/step - loss: 0.2772 - accuracy: 0.9435\n",
      "Epoch 7/20\n",
      "98/97 [==============================] - 38s 383ms/step - loss: 0.2383 - accuracy: 0.9506\n",
      "Epoch 8/20\n",
      "98/97 [==============================] - 38s 386ms/step - loss: 0.2080 - accuracy: 0.9555\n",
      "Epoch 9/20\n",
      "98/97 [==============================] - 38s 388ms/step - loss: 0.1927 - accuracy: 0.9582\n",
      "Epoch 10/20\n",
      "98/97 [==============================] - 38s 388ms/step - loss: 0.1735 - accuracy: 0.9621\n",
      "Epoch 11/20\n",
      "98/97 [==============================] - 38s 386ms/step - loss: 0.1600 - accuracy: 0.9643\n",
      "Epoch 12/20\n",
      "98/97 [==============================] - 38s 386ms/step - loss: 0.1508 - accuracy: 0.9659\n",
      "Epoch 13/20\n",
      "98/97 [==============================] - 38s 385ms/step - loss: 0.1372 - accuracy: 0.9688\n",
      "Epoch 14/20\n",
      "98/97 [==============================] - 38s 390ms/step - loss: 0.1291 - accuracy: 0.9707\n",
      "Epoch 15/20\n",
      "98/97 [==============================] - 38s 386ms/step - loss: 0.1179 - accuracy: 0.9728\n",
      "Epoch 16/20\n",
      "98/97 [==============================] - 38s 386ms/step - loss: 0.1158 - accuracy: 0.9730\n",
      "Epoch 17/20\n",
      "98/97 [==============================] - 38s 385ms/step - loss: 0.1138 - accuracy: 0.9734\n",
      "Epoch 18/20\n",
      "98/97 [==============================] - 38s 388ms/step - loss: 0.1047 - accuracy: 0.9756\n",
      "Epoch 19/20\n",
      "98/97 [==============================] - 38s 384ms/step - loss: 0.0999 - accuracy: 0.9764\n",
      "Epoch 20/20\n",
      "98/97 [==============================] - 38s 387ms/step - loss: 0.0951 - accuracy: 0.9775\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "## 1\n",
    "main_name = 'hep_LEC'\n",
    "name = main_name + '_1'\n",
    "Train(X, Y1, save_root, name, epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512])\n",
    "\n",
    "## 2\n",
    "name = main_name + '_2'\n",
    "Train(X, Y2, save_root, name, epoch = 20, batch_size = 4096, nodes = [2048, 1024, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_raw = ADoub_meta.iloc[:, 0].values.squeeze().astype(int)\n",
    "Y2_raw = ADoub_meta.iloc[:, 2].values.squeeze().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state = 0, shuffle = True)\n",
    "cv_index = kf.split(X)\n",
    "\n",
    "Accuracy = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "sta = time.time()\n",
    "\n",
    "for cv_train, cv_test in cv_index:\n",
    "    \n",
    "    i += 1\n",
    "    print(i)\n",
    "    print('c1:')\n",
    "    \n",
    "    clf = LogisticRegression(n_jobs = 24).fit(X = X[cv_train, :],\n",
    "                                              y = Y1_raw[cv_train])\n",
    "    \n",
    "    test_pred_c1 = clf.predict(X[cv_test, :])\n",
    "\n",
    "    t1 = test_pred_c1 == Y1_raw[cv_test]\n",
    "    c1_accuracy = t1.sum() / test_pred_c1.shape[0]\n",
    "    \n",
    "    print('c2:')\n",
    "    clf = LogisticRegression(n_jobs = 24).fit(X = X[cv_train, :],\n",
    "                                              y = Y2_raw[cv_train])\n",
    "    \n",
    "    test_pred_c2 = clf.predict(X[cv_test, :])\n",
    "\n",
    "    t2 = test_pred_c2 == Y2_raw[cv_test]\n",
    "    c2_accuracy = t2.sum() / test_pred_c2.shape[0]\n",
    "    \n",
    "    c12_accuracy = np.sum(t1&t2) / test_pred_c2.shape[0]\n",
    "    \n",
    "    print([c1_accuracy, c2_accuracy, c12_accuracy])\n",
    "    Accuracy.append([c1_accuracy, c2_accuracy, c12_accuracy])\n",
    "\n",
    "end = time.time()\n",
    "dura = (end - sta)/3600\n",
    "print(dura, 'h(s)')\n",
    "print(np.mean(Accuracy, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
